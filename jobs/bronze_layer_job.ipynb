{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b3a2fba-0fcd-4e06-9ca7-1f6cc7a8c5d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "flights_data_params = json.loads(\n",
    "    dbutils.widgets.get(\"input\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb849600-50d4-4880-b41c-ef5cb75649a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name, col, when, lit\n",
    "\n",
    "# Define the generic Auto Loader function\n",
    "def ingest_flights_data(table_params):\n",
    "    \"\"\"\n",
    "    Creates an Auto Loader stream for a given table configuration\n",
    "    \"\"\"\n",
    "    print(f\"Ingesting: {table_params['source_name']}...\")\n",
    "    # Auto Loader configuration\n",
    "    df = (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{table_params['checkpoint']}_schema\")\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\",\"rescue\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(table_params['raw_path'])\n",
    "        .withColumn(\"_bronze_ingested_at\", current_timestamp())\n",
    "        .withColumn(\"_source_file\", col(\"_metadata.file_path\"))\n",
    "    )\n",
    "    # Write to target Delta table\n",
    "    return (df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", table_params['checkpoint'])\n",
    "        .outputMode(\"append\")\n",
    "        .trigger(availableNow=True) # or trigger(once=True)\n",
    "        .toTable(table_params['target_table']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27039fd1-57c7-410b-be4b-38e79c64162a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_flights_data(flights_data_params)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_layer_job",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
