{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "897c6f49-3214-4baa-8a2d-5f87d3ec8092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dcafc3e-1193-4acc-9b9f-9b6f2da54c0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name, col, when, lit\n",
    "\n",
    "# Define the generic Auto Loader function\n",
    "def ingest_flights_data(table_params):\n",
    "    \"\"\"\n",
    "    Creates an Auto Loader stream for a given table configuration\n",
    "    \"\"\"\n",
    "    print(f\"Ingesting: {table_params['source_name']}...\")\n",
    "    # Auto Loader configuration\n",
    "    df = (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{table_params['checkpoint']}_schema\")\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\",\"rescue\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(table_params['raw_path'])\n",
    "        .withColumn(\"_bronze_ingested_at\", current_timestamp())\n",
    "        .withColumn(\"_source_file\", col(\"_metadata.file_path\"))\n",
    "    )\n",
    "    # Write to target Delta table\n",
    "    return (df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", table_params['checkpoint'])\n",
    "        .outputMode(\"append\")\n",
    "        .trigger(availableNow=True) # or trigger(once=True)\n",
    "        .toTable(table_params['target_table']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dddaa9f-834c-43e7-815e-d8fce22e1472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parallel execution for bronze datasets ingestion:\n",
    "queries = []\n",
    "for table_name, config in flights_data_params.items():\n",
    "    query = ingest_flights_data(config)\n",
    "    queries.append(query)\n",
    "\n",
    "# Wait for all streams to complete\n",
    "for query in queries:\n",
    "    query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
